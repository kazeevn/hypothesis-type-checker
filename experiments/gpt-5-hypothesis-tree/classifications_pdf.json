[
  {
    "paper_id": "2aKHuXdr7Q",
    "paper_title": "Going Deeper into Locally Differentially Private Graph Neural Networks",
    "hypotheses": {
      "subsidiary_hypotheses": [
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "“Reducing the effective feature dimension and expanding the effective neighborhood size help minimize the estimation error and thus enhance the utility.”",
          "epistemic_type": "causal",
          "epistemic_justification": "The statement predicts that manipulating two variables (effective feature dimension d and effective neighborhood size |N(v)|) will cause a reduction in aggregation estimation error, thereby improving utility.",
          "structural_type": "complex",
          "variables_identified": [
            "effective feature dimension d",
            "effective neighborhood size |N(v)|",
            "aggregation estimation error ξ",
            "learning utility (e.g., node classification accuracy)",
            "privacy budget ε"
          ],
          "predictive_type": "directional",
          "predicted_direction": "Decreasing effective d and increasing effective |N(v)| reduce estimation error and increase utility.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "other",
          "specific_type_details": "Design principle derived from theoretical bound (Theorem 3).",
          "confidence_score": 0.9,
          "notes": "Backed by Theorem 3 (Eq. 6) and used to motivate NFR and HOA.",
          "evaluation_status": "supported",
          "evaluation_details": "Theorem 3 derives max ξ_i = O( sqrt(d log(d/δ)) / (ε sqrt(|N(v)|)) ), analytically supporting the predicted directions; empirical results (Figs. 5–6) further show utility gains when NFR (reducing d) and HOA (expanding |N|) are applied."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "Node Feature Regularization (NFR) based on L1-regularization can perform effective feature selection on perturbed node features and improve utility, especially under small ε.",
          "epistemic_type": "causal",
          "epistemic_justification": "Claims that applying NFR causes sparsification/feature selection (lower effective d) which in turn improves accuracy.",
          "structural_type": "simple",
          "variables_identified": [
            "use of NFR layer (on/off)",
            "effective feature dimension",
            "privacy budget ε",
            "test accuracy"
          ],
          "predictive_type": "directional",
          "predicted_direction": "Turning on NFR reduces effective dimension and increases node classification accuracy; the gain is larger when ε is small.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "comparative_performance",
          "specific_type_details": "NFR-augmented mechanisms (MBM⋆, PM⋆) vs. base mechanisms (MBM, PM).",
          "confidence_score": 0.92,
          "notes": "Formalized by Thm. 5 (soft-thresholding solution) and Thm. 6/7; tested in Table 2 and Fig. 5.",
          "evaluation_status": "supported",
          "evaluation_details": "Across Cora/CiteSeer/LastFM/Facebook, MBM⋆ improves accuracy over MBM by ~2–7% depending on ε (Table 2); gains are larger at ε=0.01 (e.g., +6.8% on Cora, +6.3% on Facebook). Fig. 5 shows improvement decreases as ε increases."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "High-Order Aggregator (HOA) mitigates over-smoothing and reduces noise-bias injection compared to simple K-hop aggregation (SKA), thereby improving utility as K increases.",
          "epistemic_type": "causal",
          "epistemic_justification": "Asserts that choosing HOA instead of SKA causes better denoising and higher accuracy via personalized weighting and Dirichlet energy behavior.",
          "structural_type": "simple",
          "variables_identified": [
            "aggregation method (HOA vs SKA)",
            "K (number of hops/steps)",
            "Dirichlet energy",
            "test accuracy"
          ],
          "predictive_type": "directional",
          "predicted_direction": "HOA sustains or increases accuracy with larger K and outperforms SKA; SKA degrades at large K due to over-smoothing.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "comparative_performance",
          "specific_type_details": "Method comparison on the same datasets under LDP.",
          "confidence_score": 0.92,
          "notes": "Backed by Theorem 4 (energy ratio Φ_K→0) and extensive ablations in Fig. 6; also heterophilic graphs in Fig. 8.",
          "evaluation_status": "supported",
          "evaluation_details": "Fig. 6: On all four datasets at ε=0.01, HOA accuracy steadily increases with K while SKA peaks at small K and then drops sharply (e.g., LastFM falls <40% by K=64). HOA > SKA for K∈{2,4,8,16,32,64}."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "“Let Φ_K = lim_{K→∞} (∑_{k=1}^K Υ^k_HOA / ∑_{k=1}^K Υ^k_SKA) = 0”; hence, HOA is less influenced by infinite-hop receptive fields than SKA.",
          "epistemic_type": "associative",
          "epistemic_justification": "Mathematical property relating the Dirichlet energies of two methods; implies systematic difference without claiming experimental causation beyond the model.",
          "structural_type": "simple",
          "variables_identified": [
            "Dirichlet energy Υ^k for HOA",
            "Dirichlet energy Υ^k for SKA",
            "K"
          ],
          "predictive_type": "directional",
          "predicted_direction": "Energy ratio tends to 0 as K→∞.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "other",
          "specific_type_details": "Analytical property of the proposed operator.",
          "confidence_score": 0.88,
          "notes": "Presented as Theorem 4; proof sketch in App. B.3.",
          "evaluation_status": "supported",
          "evaluation_details": "Formally proved (Theorem 4); empirical trends in Fig. 6 are consistent with reduced over-smoothing."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "“Given δ>0 … max_i ξ_i = O( sqrt(d log(d/δ)) / (ε sqrt(|N(v)|)) ).” Therefore, feature dimension d and neighborhood size |N(v)| are the key factors governing aggregation estimation error.",
          "epistemic_type": "associative",
          "epistemic_justification": "Analytical bound shows systematic dependency between variables and error; does not claim empirical causation outside the model assumptions.",
          "structural_type": "complex",
          "variables_identified": [
            "feature dimension d",
            "neighborhood size |N(v)|",
            "privacy budget ε",
            "estimation error ξ",
            "confidence parameter δ"
          ],
          "predictive_type": "directional",
          "predicted_direction": "Error increases with d and decreases with |N(v)| and ε (per the bound).",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "other",
          "specific_type_details": "Theoretical factor analysis (Theorem 3).",
          "confidence_score": 0.9,
          "notes": "Used to motivate NFR and HOA designs.",
          "evaluation_status": "supported",
          "evaluation_details": "Bound derived via Bernstein’s inequality (App. B.2); subsequent experiments align with the implications."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "Under σ=0 and a linear AGGREGATE, the neighborhood aggregation over locally perturbed features is an unbiased estimator of the clean aggregation.",
          "epistemic_type": "descriptive",
          "epistemic_justification": "Claims a property of the estimator under specified conditions (no directionality across variables in a predictive sense).",
          "structural_type": "simple",
          "variables_identified": [
            "bias term σ",
            "AGGREGATE linearity",
            "expected aggregated embedding"
          ],
          "predictive_type": "non_directional",
          "predicted_direction": null,
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "other",
          "specific_type_details": "Estimator property (Theorem 2).",
          "confidence_score": 0.86,
          "notes": "Supports pipeline correctness.",
          "evaluation_status": "supported",
          "evaluation_details": "Formally proved as Theorem 2 (App. B.1)."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "Between the two proposed orderings, N-H (NFR→HOA) yields slightly higher accuracy than H-N when ε is small; their performance converges as ε increases.",
          "epistemic_type": "associative",
          "epistemic_justification": "Posits a systematic relationship between architecture ordering and accuracy contingent on ε, without strict causal mechanism beyond observed trends.",
          "structural_type": "complex",
          "variables_identified": [
            "architecture order (N-H vs H-N)",
            "privacy budget ε",
            "test accuracy"
          ],
          "predictive_type": "directional",
          "predicted_direction": "N-H > H-N at small ε; difference shrinks as ε grows.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "comparative_performance",
          "specific_type_details": "Two configurations of the same framework compared.",
          "confidence_score": 0.78,
          "notes": "Observed on Cora in Fig. 4(b).",
          "evaluation_status": "supported",
          "evaluation_details": "Fig. 4(b): N-H slightly outperforms H-N at ε∈{0.01,0.1}; curves align at larger ε."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "GAT degrades more than GCN/GraphSAGE under strong perturbation (small ε) because attention is more sensitive to noisy features; the gap narrows when ε≥0.1.",
          "epistemic_type": "associative",
          "epistemic_justification": "States an observed performance relationship across models conditioned on ε, with a qualitative explanation.",
          "structural_type": "complex",
          "variables_identified": [
            "backbone model (GAT vs GCN vs GraphSAGE)",
            "privacy budget ε",
            "test accuracy"
          ],
          "predictive_type": "directional",
          "predicted_direction": "At ε≈0.01, GAT < {GCN, GraphSAGE}; differences diminish for ε≥0.1.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "comparative_performance",
          "specific_type_details": "Backbone sensitivity under LDP noise.",
          "confidence_score": 0.75,
          "notes": "Empirical trend only; no formal proof.",
          "evaluation_status": "supported",
          "evaluation_details": "Fig. 4(a) shows lower accuracy for GAT at ε=0.01 on Cora; gaps reduce as ε increases."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "NFR is more effective than Dropout and Group Lasso at preserving learning utility under LDP because it performs precise feature selection for denoising.",
          "epistemic_type": "causal",
          "epistemic_justification": "Method choice (regularizer) is posited to cause utility differences via more appropriate sparsification for LDP-denoising.",
          "structural_type": "simple",
          "variables_identified": [
            "regularization method (NFR vs Dropout vs Group Lasso)",
            "test accuracy",
            "privacy budget ε"
          ],
          "predictive_type": "directional",
          "predicted_direction": "NFR > Dropout and Group Lasso in accuracy under LDP.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "comparative_performance",
          "specific_type_details": "Regularizer comparison without HOA.",
          "confidence_score": 0.8,
          "notes": "Backed by Table 4.",
          "evaluation_status": "supported",
          "evaluation_details": "At ε=0.01 (GCN), NFR achieves 71.3/57.2/67.1/84.9% on Cora/CiteSeer/LastFM/Facebook vs Dropout 63.4/52.7/61.3/77.6 and Group Lasso 57.6/50.5/57.1/78.9."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "HOA provides better denoising/utility than Residual Connections in private graph learning.",
          "epistemic_type": "causal",
          "epistemic_justification": "Claims the HOA design causes higher accuracy than adding residual connections under the same LDP conditions.",
          "structural_type": "simple",
          "variables_identified": [
            "aggregation enhancement (HOA) vs residual connections",
            "K",
            "test accuracy"
          ],
          "predictive_type": "directional",
          "predicted_direction": "HOA > Residual Connections across K.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "comparative_performance",
          "specific_type_details": "Ablation vs RC on Cora.",
          "confidence_score": 0.72,
          "notes": "Single-dataset/single-ε evidence.",
          "evaluation_status": "supported",
          "evaluation_details": "Fig. 9 (Cora, ε=0.01): HOA shows materially higher accuracy than RC for K≥2."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "UPGNET remains effective on heterophilic graphs and outperforms baselines there; HOA maintains superiority over SKA on these graphs.",
          "epistemic_type": "causal",
          "epistemic_justification": "Applies the method to a new graph regime (heterophily) and asserts it still causes utility improvements vs baselines.",
          "structural_type": "simple",
          "variables_identified": [
            "graph type (heterophilic vs homophilic)",
            "method (UPGNET vs baselines; HOA vs SKA)",
            "test accuracy",
            "privacy budget ε"
          ],
          "predictive_type": "directional",
          "predicted_direction": "On Flickr and Reddit, UPGNET outperforms BASE/LPGNN/Solitude; HOA > SKA across K.",
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "transferability",
          "specific_type_details": "Transfer to heterophilic datasets.",
          "confidence_score": 0.78,
          "notes": "Evidence in App. F.4 (Figs. 7–8).",
          "evaluation_status": "supported",
          "evaluation_details": "Fig. 7: UPGNET achieves best accuracy across ε on Flickr/Reddit; Fig. 8: HOA consistently ≥ SKA across K on both datasets."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "The entire UPGNET training and inference pipeline preserves ε-LDP due to the post‑processing property when PM or MBM is used once on client features.",
          "epistemic_type": "descriptive",
          "epistemic_justification": "A property claim about privacy guarantees given the protocol structure.",
          "structural_type": "simple",
          "variables_identified": [
            "use of PM/MBM once on client",
            "post-processing property",
            "ε-LDP guarantee"
          ],
          "predictive_type": "non_directional",
          "predicted_direction": null,
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "implementation",
          "specific_type_details": "Correctness of privacy guarantee by design.",
          "confidence_score": 0.85,
          "notes": "Standard DP result referenced; no empirical test needed.",
          "evaluation_status": "supported",
          "evaluation_details": "Formal argument in Sec. 3.2.4 and App. D referencing post-processing theorem (Dwork et al., 2014)."
        },
        {
          "subsidiary_hypotheses": null,
          "is_main": false,
          "hypothesis_text": "UPGNET’s computational complexity scales linearly with graph size and feature dimension: O(K·|E|·d + |V|·d), hence it is practical and scalable.",
          "epistemic_type": "descriptive",
          "epistemic_justification": "Describes algorithmic complexity behavior rather than a causal relation tested by experiment.",
          "structural_type": "simple",
          "variables_identified": [
            "|V|",
            "|E|",
            "d",
            "K",
            "computational cost"
          ],
          "predictive_type": "non_directional",
          "predicted_direction": null,
          "functional_type": "scientific",
          "temporal_type": "confirmatory",
          "specific_type": "implementation",
          "specific_type_details": "Asymptotic complexity analysis.",
          "confidence_score": 0.8,
          "notes": "Analytical only; no runtime benchmarks reported.",
          "evaluation_status": "supported",
          "evaluation_details": "Derived in Sec. 3.2.4 and App. E; compares favorably to baselines up to an extra |V|·d term."
        }
      ],
      "is_main": true,
      "hypothesis_text": "“Extensive experiments on real datasets demonstrate that UPGNET excels in achieving privacy preservation and superior graph learning utility.” In particular, integrating NFR and HOA with LDP protocols yields higher node‑classification accuracy than prior locally private GNN methods under the same ε.",
      "epistemic_type": "causal",
      "epistemic_justification": "Asserts that using the proposed method causes improved utility compared with baselines at fixed privacy budgets.",
      "structural_type": "complex",
      "variables_identified": [
        "method (UPGNET vs BASE, LPGNN, Solitude)",
        "privacy budget ε",
        "dataset (Cora, CiteSeer, LastFM, Facebook; plus Flickr, Reddit)",
        "backbone (GCN, GraphSAGE, GAT)",
        "test accuracy"
      ],
      "predictive_type": "directional",
      "predicted_direction": "UPGNET produces higher accuracy than baselines at the same ε while preserving ε-LDP.",
      "functional_type": "scientific",
      "temporal_type": "confirmatory",
      "specific_type": "comparative_performance",
      "specific_type_details": "Across datasets and backbones in Figs. 3–4.",
      "confidence_score": 0.93,
      "notes": "Privacy preservation is by design (post-processing); main empirical novelty is utility gains.",
      "evaluation_status": "supported",
      "evaluation_details": "Fig. 3: UPGNET consistently outperforms BASE/LPGNN/Solitude across ε on four datasets and three backbones; on Facebook, accuracy approaches NonPriv. Fig. 4(a) shows robustness across backbones. Privacy guarantee relies on single-shot PM/MBM + post-processing."
    },
    "source_mode": "pdf",
    "processing_notes": "I scanned the full paper and appendices. Hypotheses include theoretical properties (Theorems 2–6/7), design predictions (role of d and |N|), and empirical comparative claims (overall gains, NFR/HOA ablations, architecture ordering, backbone sensitivity, heterophily transfer). Duplicates across sections were consolidated. Where claims were purely theoretical, evaluation status cites proofs; where empirical, figures/tables are referenced. The paper does not report runtime experiments; complexity and privacy claims are treated as analytically supported."
  }
]